# -*- coding: utf-8 -*-
"""Athaya-Dicoding-Submission1-WaterPotability.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0Cdhu6-qdfTWCJKBQtrCJS9vsBwGL8I

# Aman untuk Diminum
## Melatih Model *Machine Learning* untuk Mengenal Air yang Aman Diminum

### Pemerolehan Dataset dari Kaggle

1. Install Kaggle pada Colab
"""

pip install kaggle

"""2. Unduh Kaggle API dari menu *Settings*, kemudian unggah ke Colab melalui fitur *Files* yang ada di kiri layar.
3. Dokumentasi ini menggunakan kode berikut untuk membuat *prompt* unggah data dan memindahkan data ke `/.kaggle`
"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('Pengguna mengunggah file "{name}" dengan rentang {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"""4. Mengunduh dataset `water_potability.zip` dari Kaggle
5. `Unzip` dataset dan disimpan di `/content`
"""

!kaggle datasets download -d adityakadiwal/water-potability

!unzip water-potability.zip

"""### Pemahaman terhadap Data dengan Analisis Data Eksploratori dan Preproses Data

1. *Import* terhadap *libraries* yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

"""2. Memindahkan dataset `.csv` ke dalam DataFrame. Pada saat ini, *dataset* yang telah diunduh dari Kaggle terletak di `/content/water_potability.csv`"""

df = pd.read_csv('/content/water_potability.csv')

"""3. Pemeriksaan Dimensi Dataset"""

# Dimensi dataset
df.shape

"""Hasil dari `df.shape` menunjukkan bahwa dataset terdiri dari 3276 sampel (baris, *row*) yang tersebar pada 10 fitur (kolom, *column*, *features*)

4. Selayang pandang dataset: 4 sampel teratas dan tipe data yang digunakan.
"""

# Selayang pandang dataset
df.head()

# Tipe data tiap fitur/kolom
df.dtypes

"""5. Pemeriksaan *Null Values* dan Resolusi Isu tersebut menggunakan *Random Forest*"""

# Pemeriksaan nilai kosong (null value)
df.isnull().sum()

"""Dalam tahap pemeriksaan *null values* ditemukan beberapa *null values* yang tersebar di variabel `ph`, `Sulfate`, dan `Trihalomethanes`. Untuk memeriksa apakah ada pola tertentu dalam kemunculan *null values* ini, akan digunakan visualisasi."""

# Visualisasi null values
missing_data = df[['ph', 'Sulfate', 'Trihalomethanes']].isnull()
sns.heatmap(missing_data, cbar=False, cmap='viridis')

# Legenda
legend_labels = ['Tidak Hilang', 'Hilang']
color_map = [sns.color_palette('viridis')[0], sns.color_palette('viridis')[-1]]
legend_handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in color_map]
plt.legend(legend_handles, legend_labels, bbox_to_anchor=(1.00, 1), loc='upper left')

plt.show()

"""Hasil menunjukkan bahwa *null values* (warna kuning) tidak muncul mengikuti pola tertentu, namun hanya muncul tersebardi beberapa tingkat. Sehingga untuk mengisi nilai-nilai kosong tersebut dapat digunakan metode Imputasi.

Untuk kasus ini, digunakan Imputasi berbasis Model dengan Model *Random Forest*. Digunakan demikian karena dataset ini akan menghasilkan nilai prediktif pada potabilitas (keterminuman) air, sehingga memiliki hipotesa yang kuat bahwa variabel dalam dataset ini saling berkorelasi satu sama lain.
"""

# Library yang ditambah (dipindah ke atas)

# from sklearn.ensemble import RandomForestRegressor
# from sklearn.impute import SimpleImputer

# Untuk keamanan, uji coba pengisian dataset dilakukan pada
# variabel yang berbeda yang merupakan salinan df

df_imputed = df.copy()

# Fitur yang akan digunakan pada metode Imputasi
features = ['Hardness', 'Solids', 'Chloramines', 'Conductivity', 'Organic_carbon', 'Turbidity']

# Iterasi pada kolom dengan nilai yang hilang
for column in ['ph', 'Sulfate', 'Trihalomethanes']:

    # Membuat mask untuk mengidentifikasi null values
    mask = df_imputed[column].isnull()

    # Memecah dataset kepada complete dan incomplete
    complete_cases = df_imputed.loc[~mask, features + [column]]
    incomplete_cases = df_imputed.loc[mask, features + [column]]

    # Memecah complete cases kepada fitur dan target
    X_train = complete_cases[features]
    y_train = complete_cases[column]

    # Fitting random forest regressor
    rf = RandomForestRegressor()
    rf.fit(X_train, y_train)

    # Menggunakan regressor untuk mengisi null values
    X_impute = incomplete_cases[features]
    y_impute = rf.predict(X_impute)

    # Menggunakan identifikasi mask, hasil regressor di masukkan ke
    # dalam dataframe salinan
    df_imputed.loc[mask, column] = y_impute

# Tampilkan untuk pengecekan apakah semua null values sudah terisi
print(df_imputed.isnull().sum())

"""Dapat terlihat dari hasil ini, bahwa *null values* sudah tidak ditemukan pada dataset. Perlu diingat bahwa kita melakukan penyalinan dari variabel `df` ke `df_imputed` sehingga perlu dilakukan penulisan ulang dari variabel `df`."""

# Menggantikan nilai pada variabel df menjadi berisi nilai pada variabel df_imputed
df = df_imputed

"""6. Deskripsi statistik dataset setelah dilakukan proses imputasi"""

df.describe()

"""Dari hasil `df.describe()` dapat dilihat bahwa seluruh nilai `count` sudah setara (3276 sampel) yang menunjukkan bahwa *null values* sudah tidak ditemukan pada dataset.

Untuk menginterpretasikan sisa data pada hasil `df.describe()` dilakukan visualisasi fitur numerik.

7. Visualisasi Fitur Numerik
"""

# Histogram
df.hist(figsize=(12, 8))
plt.tight_layout()
plt.show()

"""Karena variabel `Solids` menunjukkan tendensi (*skewness*) ke kanan, maka dilakukan evaluasi terhadap tendensi ini.

Perlu diingat bahwa tiap feature saat ini memiliki skala yang berbeda karena menggunakan metrik pengukuran yang berbeda, hingga saat ini penanganan hanya per feature, tidak seluruhnya.

Untuk memahami *skewness* lihat [disini](https://www.scribbr.com/statistics/skewness/)
"""

# Hitung Skewness tiap fitur
skewness = df.skew()

# Tampilan skewness pada fitur
print("Nilai Skewness:")
for feature, skew in skewness.items():
    print(f"Skewness dari '{feature}': {skew}")

"""Melihat bahwa nilai *Skewness* tidak lebih dari 1, dan dekat dengan 0, seharusnya bisa diabaikan. Namun, melihat pada grafik, dapat disimpulkan bahwa fitur `Solids` adalah yang paling problematik, memiliki *skewness value* yang paling tinggi.

Untuk mengatasinya akan digunakan *Square Root Transformation*. Alasan penggunaan adalah transformasi tersebut menjaga (*retain*) urutan besaran angka.
"""

# Membuat variabel baru untuk keamanan
df_transformed = df.copy()

# Square Root Transformation pada DF
df_transformed['Solids'] = np.sqrt(df['Solids'])

# Histogram Terbaru
df_transformed.hist(figsize=(12, 8))
plt.tight_layout()
plt.show()

# Skewness terbaru
skewness = df_transformed.skew()

# Tampilkan skewness
print("Nilai skewness pd tiap fitur")
for feature, skew in skewness.items():
    print(f"Skewness dari '{feature}': {skew}")

"""Menggunakan hasil dari proses transformasi menggunakan akar kuadrat ditampilkan diatas, dapat disimpulkan bahwa *skewness* pada `Solids` dapat dikurangi."""

# Memperbarui nilai Solids pada df
df['Solids'] = df_transformed['Solids']

new_skew=df['Solids'].skew()
print(new_skew)

"""8. *Outlier* pada data: Visualisasi dengan Box Plot"""

# Set the number of columns and rows for subplots
num_cols = 3
num_rows = (len(df.columns) - 1) // num_cols + 1

# Create subplots for box plots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))

# Flatten the axes array to iterate over the subplots
axes = axes.flatten()

# Iterate over each feature and create a box plot in each subplot
for i, column in enumerate(df.columns):
    sns.boxplot(data=df, y=column, ax=axes[i])
    axes[i].set_title(f"Box Plot of {column}")
    axes[i].set_ylabel(column)

# Remove any extra subplots
if len(df.columns) < num_rows * num_cols:
    for j in range(len(df.columns), num_rows * num_cols):
        fig.delaxes(axes[j])

# Adjust the spacing between subplots
fig.tight_layout()

# Show the plot
plt.show()

"""Hingga saat ini Potability diabaikan karena hanya berisi 0 dan 1 untuk menyatakan bahwa air bisa diminum atau tidak.

Model yang akan dikembangkan akan menggunakan random forest, karena algoritma random forest andal dalam menangani outlier.

## Pengembangan Model

1. Pemecahan dataset menjadi fitur-fitur (X) dan target variabel `Potability`
"""

X = df.drop('Potability', axis=1)
y = df['Potability']

"""2. Pemecahan lanjutan menjadi variabel train dan test."""

# from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=52)

"""3. *Feature scaling* dilakukan untuk menyamakan metrik pada dataset, karena masing-masing data menggunakan metrik pengukuran yang berbeda. Fitting scaler dilakukan pada data train untuk mencegah bocornya test data ke train data."""

# from sklearn.preprocessing import StandardScaler

# Panggil StandardScaler
scaler = StandardScaler()

# Fit scaler thd train data
scaler.fit(X_train)

# Transform data train dan test
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""Untuk memeriksa apakah feature scaling berhasil, `X_train_scaled` dan `X_train` dibandingkan."""

# DataFraming
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)

# Tampilkan Statistik Deskriptif masing-masing X_train_scaled dan X_train
print("X_train_scaled Deskriptif")
print(X_train_scaled_df.describe())
print("\nX_train Deskriptif:")
print(X_train.describe())

# Histogram Solids untuk membandingkan
feature = 'Solids'
plt.figure(figsize=(8, 6))
plt.hist(X_train[feature], bins=20, color='blue', alpha=0.5, label='Data Sebelum Skaler')
plt.hist(X_train_scaled_df[feature], bins=20, color='green', alpha=0.5, label='Data setelah Skaler')

plt.title(f"Distribusi {feature}")
plt.xlabel(feature)
plt.ylabel("Frekuensi")
plt.legend()
plt.show()

"""Dari deskriptif dan contoh dari variabel `Solids`, data sudah di *scale* dengan mengurangi besar value namun tetap mempertahankan variabilitas.

4. Melatih model dengan Algoritma Random Forest.
"""

# from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=52)

# melatih model dengan X_train_scaled dan y_train

model.fit(X_train_scaled, y_train)

"""Sampai tahap ini model telah dilatih, kemudian langkah selanjutnya adalah evaluasi model dengan melakukan test.

5. Evaluasi Model
"""

# Membuat prediksi pada fitur test
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

y_pred = model.predict(X_test_scaled)

# Metrik evaluasi

# Accuracy/Keakuratan
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Confusion/Kebingungan Model
confusion_mat = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", confusion_mat)

# Classification Report/Laporan Klasifikasi
classification_rep = classification_report(y_test, y_pred)
print("Classification Report:\n", classification_rep)

"""Dari matriks pengujian diatas dapat dilihat:

* Akurasi model 67.68% ini berarti 67.68% dari sampel air pada dataset test diklasifikasikan secara akurat baik antara bisa diminum (1) atau tidak bisa diminum (0)
* Pada confusion matrix terdapat 4 nilai, nilai pertama adalah 356, yang merupakan True Positives, bermakna model mengidentifikasikan secara tepat 356 sampel sebagai air dapat diminum.
* Kedua, nilai True Negative 88, bermakna model secara akurat mengidentifikasi 88 sampel sebagai air tidak dapat diminum.
* Ketiga, nilai False Negative 46, yang bermakna model melakukan kesalahan klasifikasi 46 sampel air yang aslinya dapat diminum, menjadi diklasifikasikan sebagai air tidak dapat diminum.
* Keempat, nilai False Positive 166, berarti model melakukan kesalahan klasifikasi 166 sampel yang tidak bisa diminum, dan mengklasifikasikannya sebagai air yang bisa diminum.

## Tuning Model dengan Feature Selection

Karena akurasi model masih 67.68%, dilakukan Feature Selection. Seleksi fitur dilakukan untuk memilih fitur yang memiliki pengaruh pada target (`potability`).
"""

# Latih Model
model.fit(X_train_scaled, y_train)

# Analisis kepentingan fitur
feature_importances = model.feature_importances_

# Urutkan kepentingan fitur
indices = np.argsort(feature_importances)[::-1]

# Pilih fitur dengan urutan kepentingan top 5
k = 5
top_k_features = X_train.columns[indices][:k]

# Buat variabel baru
X_train_selected = X_train[top_k_features]
X_test_selected = X_test[top_k_features]

# Uji Ulang
# Membuat model random forest dengan n_estimators=300
model_selected = RandomForestClassifier(n_estimators=300, random_state=52)

# Latih model dengan fitur terpilih
model_selected.fit(X_train_selected, y_train)

# Prediksi
y_pred_selected = model_selected.predict(X_test_selected)

# Evaluasi
accuracy_selected = accuracy_score(y_test, y_pred_selected)
confusion_mat_selected = confusion_matrix(y_test, y_pred_selected)
classification_rep_selected = classification_report(y_test, y_pred_selected)

print("Accuracy (Selected Features):", accuracy_selected)
print("Confusion Matrix (Selected Features):\n", confusion_mat_selected)
print("Classification Report (Selected Features):\n", classification_rep_selected)

"""Dari hasil usaha tuning menggunakan seleksi fitur, tidak ditemukan peningkatan yang signifikan dari akurasi model, ini bisa terjadi karena:

* Jumlah data yang kurang
* Algoritma yang digunakan tidak tepat.

Namun perlu digaris bawahi bahwa model ini mampu mengidentifikasi hampir 68% secara akurat antara air yang bisa diminum dan tidak diminum.
"""